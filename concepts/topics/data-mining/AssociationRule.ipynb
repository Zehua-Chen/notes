{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, FrozenSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Items = List[str]\n",
    "Itemset = List[str]\n",
    "Itemsets = List[Itemset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "LHS -> RHS\n",
    "```\n",
    "\n",
    "If `LHS` is bought in a transaction then `RHS` is also likely to be bought in the transaction\n",
    "\n",
    "- `A -> B`: `A` implies `B`\n",
    "- `LHS` and `RHS` are set of items, i.e. itemsets\n",
    "- **Items are typically strings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support and Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transaction = FrozenSet[str]\n",
    "Transactions = List[Transaction]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Support for a set of items**: percentage of transactions containing all these items\n",
    "- **Confidence for a rule**: percent of transactions containing LHS that also contain RHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_support(itemset: Itemset, transactions: Transactions):\n",
    "    count = 0\n",
    "    itemset_s = frozenset(itemset)\n",
    "\n",
    "    for transaction in transactions:\n",
    "        if itemset_s.issubset(transaction):\n",
    "            count += 1\n",
    "\n",
    "    return count / float(len(transactions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding All Rules With Minimum Support and Minimum Confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation 1\n",
    "\n",
    "It is enough to \n",
    "\n",
    "- find all large (frequent) itemsets together with their support\n",
    "- derive all association rules with sufficient confidence from each large itemsets\n",
    "\n",
    "To decide whether to output `LHS -> RHS`, we need to \n",
    "\n",
    "1. Check that `LHS union (or) RHS` is large or frequent, i.e. `support(LHS union RHS) >= min_sup`\n",
    "2. Check that `confidence(LHS -> RHS) >= min_confidence`\n",
    "\n",
    "\n",
    "- number of scans is k + 1, where k is the number of items in the largest itemset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation 2: A Priori Property\n",
    "\n",
    "**A priori property**: every subset of a large (frequent) itemset is also large (frequent)\n",
    "\n",
    "- A priori algorithm find large itemsets with a minimum support\n",
    "\n",
    "```sql\n",
    "insert into c_k+1\n",
    "\n",
    "SELECT P.item1, P.item2, ... P.itemk, Q.itemk\n",
    "FROM L_k P, L_k Q\n",
    "WHERE P.item1 = Q.item1 \n",
    "  AND P.item2 = Q.item2\n",
    "  AND ...\n",
    "  AND P.itemk-1 = Q.itemk-1\n",
    "  AND P.itemk < Q.itemk\n",
    "```\n",
    "\n",
    "- P and Q agrees except the last item\n",
    "- `P.itemk < Q.itemk` keeping elements ordered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "Technically `get_frequent_itemsets` **also returns an empty set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsets_k_minus_1(itemset: Itemset) -> Itemsets:\n",
    "    subsets = []\n",
    "\n",
    "    for skip in range(len(itemset)):\n",
    "        subset = itemset[0:skip] + itemset[skip + 1:]\n",
    "        subsets.append(subset)\n",
    "\n",
    "    return subsets\n",
    "\n",
    "\n",
    "def apriori_gen(itemsets: List[List[str]]):\n",
    "    \"\"\"Given L_k-1, produce C_k\n",
    "\n",
    "    Args:\n",
    "        itemsets (Itemsets): L_k-1\n",
    "\n",
    "    Returns:\n",
    "        _type_: C_k\n",
    "    \"\"\"\n",
    "    candidates = []  # type: Itemsets\n",
    "    itemsets_s = frozenset(map(frozenset, itemsets))\n",
    "\n",
    "    for p in itemsets:\n",
    "        for q in itemsets:\n",
    "            p_q = list(zip(p, q))\n",
    "            p_q_until_k_min_2 = p_q[:len(p) - 1]\n",
    "            p_q_until_k_min_2 = map(lambda x: x[0] == x[1], p_q_until_k_min_2)\n",
    "            p_q_until_k_min_2 = all(p_q_until_k_min_2)\n",
    "\n",
    "            p_q_k_min_1 = p_q[len(p) - 1:len(p)]\n",
    "            p_q_k_min_1 = map(lambda x: x[0] < x[1], p_q_k_min_1)\n",
    "            p_q_k_min_1 = all(p_q_k_min_1)\n",
    "\n",
    "            if p_q_until_k_min_2 and p_q_k_min_1:\n",
    "                candidate = p + q[len(p) - 1:len(p)]\n",
    "                candidates.append(candidate)\n",
    "\n",
    "    def is_subsets_in_l(candidate: Itemset) -> bool:\n",
    "        for subset in subsets_k_minus_1(candidate):\n",
    "            if frozenset(subset) not in itemsets_s:\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    candidates = list(filter(is_subsets_in_l, candidates))\n",
    "\n",
    "    return candidates\n",
    "\n",
    "\n",
    "def get_frequent_itemsets(\n",
    "        items: Items, \n",
    "        transactions: Transactions, \n",
    "        min_sup: float) -> Itemsets:\n",
    "    \"\"\"Given a dataframe, compute the most frequent itemsets\n",
    "\n",
    "    Slides\n",
    "    - Lecture 10\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): _description_\n",
    "        min_sup (float): _description_\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: a dataframe with the following columns: itemsets, support\n",
    "    \"\"\"\n",
    "    assert 0 <= min_sup <= 1\n",
    "\n",
    "    candidates = list(map(lambda i: [i], items))  # type: Itemsets\n",
    "\n",
    "    def large_itemsets(itemsets: Itemsets) -> Itemsets:\n",
    "        return list(filter(\n",
    "            lambda candidate: get_support(candidate, transactions) > min_sup,\n",
    "            itemsets))\n",
    "\n",
    "    all_itemsets = large_itemsets(candidates)\n",
    "\n",
    "    while True:\n",
    "        candidates = apriori_gen(candidates)\n",
    "        candidates = large_itemsets(candidates)\n",
    "\n",
    "        if len(candidates) == 0:\n",
    "            break\n",
    "\n",
    "        all_itemsets = all_itemsets + candidates\n",
    "\n",
    "    return all_itemsets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Associations With Minimum Confidence\n",
    "\n",
    "The association of an itemset is just all the combination of elements of the itemset on LHS  and RHS\n",
    "\n",
    "- Confidence `confidence(LHS -> RHS) = support(LHS union RHS) / support(LHS)`\n",
    "- Support: `support(LHS union RHS)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_associations(transactions, frequent_itemsets: Itemsets, min_conf: float) -> list:\n",
    "    \"\"\"Given frequent itemsets, compute the associations\n",
    "    Args:\n",
    "        df (DataFrame): original dataframe generated from input file\n",
    "        frequent_itemsets (DataFrame): frequent itemsets generated from Apriori algorithm\n",
    "        min_conf (float): minimum confidence threshold\n",
    "    Returns:\n",
    "        DataFrame: a dataframe with the following columns: left, right,\n",
    "        confidence, support\n",
    "    \"\"\"\n",
    "    assert 0 <= min_conf <= 1\n",
    "\n",
    "    associations_list = []\n",
    "\n",
    "    for itemset in frequent_itemsets:\n",
    "        if len(itemset) < 2:\n",
    "            continue\n",
    "\n",
    "        for rhs in itemset:\n",
    "            lhs_temp = set(itemset)\n",
    "            lhs_temp.remove(rhs)\n",
    "            lhs = list(lhs_temp)\n",
    "            \n",
    "            supp_lhs = get_support(lhs, transactions)\n",
    "            support_union = get_support(itemset, transactions)\n",
    "\n",
    "            conf = support_union / supp_lhs\n",
    "\n",
    "            if conf >= min_conf:\n",
    "                associations_list.append({\n",
    "                    \"left\": lhs, \n",
    "                    \"right\": [rhs], \n",
    "                    \"confidence\": conf, \n",
    "                    \"support\": support_union})\n",
    "\n",
    "    return associations_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative Association Rules\n",
    "\n",
    "When items are numbers\n",
    "\n",
    "- **Problem**: cannot use algorithm for string support rules; not enough support\n",
    "- **Solution**: bucketizing; define an item as a range of values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('research')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
