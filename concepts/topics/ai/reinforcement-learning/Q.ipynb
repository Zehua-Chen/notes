{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_q(state_size: int = 3, action_size: int = 2):\n",
    "    return np.zeros((state_size, action_size))\n",
    "\n",
    "\n",
    "create_q()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Q\" stands for quality, i.e. how useful a given action is in gaining some future\n",
    "reward\n",
    "\n",
    "- `Q` function can be implemented as a table\n",
    "- Q learning is \n",
    "  - **Model free**\n",
    "  - **Value based**\n",
    "  - **Off policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \"\"\"Our game is split into three states, 0, 1, 2. The player starts at state\n",
    "    0, and our goal is to move the player to state 2\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        self.state = 0\n",
    "\n",
    "    def _assert_state(self):\n",
    "        state = self.state\n",
    "        assert state == 0 or state == 1 or state == 2\n",
    "\n",
    "    def next_state(self, action):\n",
    "        state = self.state \n",
    "\n",
    "        if action == 0:\n",
    "            return np.clip(state - 1, 0, 2)\n",
    "        \n",
    "        if action == 1:\n",
    "            return np.clip(state + 1, 0, 2)\n",
    "\n",
    "        assert False\n",
    "\n",
    "    def reward(self, state) -> float:\n",
    "        if state == 0:\n",
    "            return -2.0\n",
    "\n",
    "        if state == 1:\n",
    "            return -1.0\n",
    "        \n",
    "        return 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q agent can perform two actions\n",
    "\n",
    "- Explore\n",
    "- Exploit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random = 0.5488135039273248\n",
      "exploit\n"
     ]
    }
   ],
   "source": [
    "def simple_train():\n",
    "    epsilon = 0.5\n",
    "\n",
    "    random = np.random.random((1,))\n",
    "    print(f\"random = {random[0]}\")\n",
    "\n",
    "    if random[0] < epsilon:\n",
    "        print(\"explore\")\n",
    "    else:\n",
    "        print(\"exploit\")\n",
    "\n",
    "\n",
    "simple_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can balance exploration and exploitation using $ \\epsilon $ (epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def exploit(q: np.ndarray, state: int) -> np.int64:\n",
    "    action_values = q[state]\n",
    "    return np.argmax(action_values) \n",
    "\n",
    "\n",
    "exploit(np.array([[0, 1]]), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find all possible action for a given state, then the agent select the action\n",
    "based on the max value of these action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def explore():\n",
    "    actions = np.array([0, 1], dtype=np.int64)\n",
    "    return np.random.choice(actions)\n",
    "\n",
    "\n",
    "explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select action at random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating Q Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q(q: np.ndarray, state, new_state, action, reward):\n",
    "    lr = 0.5\n",
    "    gamma = 0.85\n",
    "\n",
    "    q[state, action] = q[state, action] \\\n",
    "        + lr * (reward + gamma * np.max(q[new_state, :]) - q[state, action]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Agent start in a state (s1), take action (a1) and get reward (r1)\n",
    "2. Agent select the action using Q table or by random\n",
    "3. Update Q values\n",
    "\n",
    "Update occurs after each step or action and ends when an episode is done.\n",
    "\n",
    "- \"done\" means reaching some terminal point by the agent\n",
    "- Terminate state can be reaching the end of some game\n",
    "\n",
    "Agent will not learn much after each episode, but with enough exploring, will \n",
    "converge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Learning Rate** (`lr`): aka **alpha** (`a`), how much you accept a new value\n",
    "  vs the old value\n",
    "- **Gamma** (`gamma`): a discount factor. Balance immediate and future reward.\n",
    "  Value is typically in the range (`0.8` to `0.99`)\n",
    "- **Reward**: reward is the value received after completing a certain action\n",
    "- **Max**: take the maxium of the future reward and applying to the reward \n",
    "  for the current state. What this does is impact the current action by the \n",
    "  possible future reward. This is the beauty of q-learning. Weâ€™re allocating \n",
    "  future reward to current actions to help the agent select the highest return \n",
    "  action at any given state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting Everything Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.2354126 , -0.24272949],\n",
       "       [-1.        ,  0.89912109],\n",
       "       [ 0.        ,  0.        ]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train(epochs: int = 10, epsilon: float = 0.7) -> np.ndarray:\n",
    "    q = create_q()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        env = Environment()\n",
    "        step = 0\n",
    "\n",
    "        # print(f\"epoch {epoch}\")\n",
    "\n",
    "        while True:\n",
    "            # print(f\"  step {step}\")\n",
    "            train_explore = np.random.random((1,))\n",
    "\n",
    "            if train_explore[0] < epsilon:\n",
    "                # explore\n",
    "                # print(f\"    explore\")\n",
    "                action = exploit(q, env.state) \n",
    "            else:\n",
    "                # exploit\n",
    "                # print(f\"    exploit\")\n",
    "                action = explore() \n",
    "\n",
    "            old_state = env.state\n",
    "            new_state = env.next_state(action)\n",
    "\n",
    "            update_q(q, old_state, new_state, action, env.reward(new_state))\n",
    "\n",
    "            # print(f\"    {old_state} to {new_state}\")\n",
    "            env.state = new_state\n",
    "\n",
    "            step += 1\n",
    "\n",
    "            if env.state == 2:\n",
    "                break \n",
    "\n",
    "    return q\n",
    "\n",
    "\n",
    "trained_q = train()\n",
    "trained_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "- [Simple Reinforcement Learning: Q-learning](https://towardsdatascience.com/simple-reinforcement-learning-q-learning-fcddc4b6fe56)\n",
    "- [A Beginners Guide to Q-Learning](https://towardsdatascience.com/a-beginners-guide-to-q-learning-c3e2a30a653c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('coms4995')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
